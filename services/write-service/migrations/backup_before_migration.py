#!/usr/bin/env python3
"""
è¿ç§»å‰æ•°æ®åº“å¤‡ä»½è„šæœ¬

åœ¨æ‰§è¡ŒSPU+SKUæ¨¡å‹è¿ç§»å‰ï¼Œå¤‡ä»½æ‰€æœ‰ç›¸å…³çš„åŸå§‹æ•°æ®
ç¡®ä¿åœ¨è¿ç§»å¤±è´¥æ—¶å¯ä»¥å®Œå…¨æ¢å¤åˆ°åŸå§‹çŠ¶æ€

å¤‡ä»½ç­–ç•¥ï¼š
1. åˆ›å»ºæ—§è¡¨çš„å®Œæ•´å‰¯æœ¬
2. å¯¼å‡ºæ•°æ®åˆ°æ–‡ä»¶
3. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
"""

import asyncio
import logging
import json
import os
from typing import Dict, List
from sqlalchemy import create_engine, text
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from datetime import datetime

from app.core.config import settings

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PreMigrationBackup:
    """è¿ç§»å‰å¤‡ä»½å·¥å…·"""
    
    def __init__(self, database_url: str):
        self.engine = create_async_engine(database_url)
        self.SessionLocal = sessionmaker(
            bind=self.engine, 
            class_=AsyncSession, 
            expire_on_commit=False
        )
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.backup_dir = f"backups/migration_backup_{self.timestamp}"
    
    def create_backup_directory(self):
        """åˆ›å»ºå¤‡ä»½ç›®å½•"""
        os.makedirs(self.backup_dir, exist_ok=True)
        logger.info(f"ğŸ“ å¤‡ä»½ç›®å½•: {self.backup_dir}")
    
    async def backup_table_structure(self, session: AsyncSession, table_name: str):
        """å¤‡ä»½è¡¨ç»“æ„"""
        logger.info(f"ğŸ—ï¸ å¤‡ä»½è¡¨ç»“æ„: {table_name}")
        
        # è·å–è¡¨ç»“æ„
        result = await session.execute(text(f"""
            SELECT column_name, data_type, is_nullable, column_default
            FROM information_schema.columns
            WHERE table_name = '{table_name}'
            ORDER BY ordinal_position
        """))
        
        columns = []
        for row in result.fetchall():
            columns.append({
                'name': row.column_name,
                'type': row.data_type,
                'nullable': row.is_nullable,
                'default': row.column_default
            })
        
        # ä¿å­˜ç»“æ„ä¿¡æ¯
        structure_file = os.path.join(self.backup_dir, f"{table_name}_structure.json")
        with open(structure_file, 'w', encoding='utf-8') as f:
            json.dump({
                'table_name': table_name,
                'columns': columns,
                'backup_time': self.timestamp
            }, f, indent=2, ensure_ascii=False, default=str)
        
        return len(columns)
    
    async def backup_table_data(self, session: AsyncSession, table_name: str):
        """å¤‡ä»½è¡¨æ•°æ®"""
        logger.info(f"ğŸ’¾ å¤‡ä»½è¡¨æ•°æ®: {table_name}")
        
        try:
            # è·å–æ‰€æœ‰æ•°æ®
            result = await session.execute(text(f"SELECT * FROM {table_name}"))
            rows = result.fetchall()
            columns = result.keys()
            
            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            data = []
            for row in rows:
                row_dict = {}
                for i, col in enumerate(columns):
                    value = row[i]
                    # å¤„ç†ç‰¹æ®Šç±»å‹
                    if isinstance(value, datetime):
                        value = value.isoformat()
                    row_dict[col] = value
                data.append(row_dict)
            
            # ä¿å­˜æ•°æ®
            data_file = os.path.join(self.backup_dir, f"{table_name}_data.json")
            with open(data_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'table_name': table_name,
                    'row_count': len(data),
                    'columns': list(columns),
                    'data': data,
                    'backup_time': self.timestamp
                }, f, indent=2, ensure_ascii=False, default=str)
            
            return len(data)
        
        except Exception as e:
            logger.warning(f"âš ï¸ å¤‡ä»½è¡¨ {table_name} æ•°æ®å¤±è´¥: {e}")
            return 0
    
    async def create_table_backup_copy(self, table_name: str):
        """åˆ›å»ºè¡¨çš„å‰¯æœ¬"""
        backup_table_name = f"{table_name}_backup_{self.timestamp}"
        
        try:
            async with self.engine.begin() as conn:
                # åˆ›å»ºè¡¨å‰¯æœ¬
                await conn.execute(text(f"""
                    CREATE TABLE {backup_table_name} AS 
                    SELECT * FROM {table_name}
                """))
                
                logger.info(f"âœ… åˆ›å»ºè¡¨å‰¯æœ¬: {table_name} â†’ {backup_table_name}")
                return backup_table_name
        
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å‰¯æœ¬å¤±è´¥ {table_name}: {e}")
            return None
    
    async def backup_indexes_and_constraints(self, session: AsyncSession, table_name: str):
        """å¤‡ä»½ç´¢å¼•å’Œçº¦æŸä¿¡æ¯"""
        logger.info(f"ğŸ”— å¤‡ä»½ç´¢å¼•å’Œçº¦æŸ: {table_name}")
        
        # è·å–ç´¢å¼•ä¿¡æ¯
        indexes_result = await session.execute(text(f"""
            SELECT indexname, indexdef 
            FROM pg_indexes 
            WHERE tablename = '{table_name}'
        """))
        indexes = [{'name': row.indexname, 'definition': row.indexdef} 
                  for row in indexes_result.fetchall()]
        
        # è·å–çº¦æŸä¿¡æ¯
        constraints_result = await session.execute(text(f"""
            SELECT conname, pg_get_constraintdef(c.oid) as definition
            FROM pg_constraint c
            JOIN pg_class t ON c.conrelid = t.oid
            WHERE t.relname = '{table_name}'
        """))
        constraints = [{'name': row.conname, 'definition': row.definition}
                      for row in constraints_result.fetchall()]
        
        # ä¿å­˜ç´¢å¼•å’Œçº¦æŸä¿¡æ¯
        metadata_file = os.path.join(self.backup_dir, f"{table_name}_metadata.json")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump({
                'table_name': table_name,
                'indexes': indexes,
                'constraints': constraints,
                'backup_time': self.timestamp
            }, f, indent=2, ensure_ascii=False)
        
        return len(indexes) + len(constraints)
    
    async def verify_backup(self, session: AsyncSession, table_name: str):
        """éªŒè¯å¤‡ä»½å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥åŸè¡¨è¡Œæ•°
            result = await session.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
            original_count = result.fetchone()[0]
            
            # æ£€æŸ¥å¤‡ä»½è¡¨è¡Œæ•°
            backup_table = f"{table_name}_backup_{self.timestamp}"
            result = await session.execute(text(f"SELECT COUNT(*) FROM {backup_table}"))
            backup_count = result.fetchone()[0]
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶
            data_file = os.path.join(self.backup_dir, f"{table_name}_data.json")
            if os.path.exists(data_file):
                with open(data_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    file_count = data['row_count']
            else:
                file_count = 0
            
            success = (original_count == backup_count == file_count)
            
            if success:
                logger.info(f"âœ… å¤‡ä»½éªŒè¯æˆåŠŸ: {table_name} ({original_count} è¡Œ)")
            else:
                logger.warning(f"âš ï¸ å¤‡ä»½éªŒè¯å¤±è´¥: {table_name}")
                logger.warning(f"   åŸè¡¨: {original_count}, å¤‡ä»½è¡¨: {backup_count}, æ–‡ä»¶: {file_count}")
            
            return success
        
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½éªŒè¯å¤±è´¥ {table_name}: {e}")
            return False
    
    async def create_backup_manifest(self, backup_info: Dict):
        """åˆ›å»ºå¤‡ä»½æ¸…å•"""
        manifest = {
            'backup_time': self.timestamp,
            'backup_directory': self.backup_dir,
            'database_url': settings.DATABASE_URL.split('@')[1] if '@' in settings.DATABASE_URL else 'masked',
            'tables_backed_up': backup_info,
            'total_tables': len(backup_info),
            'total_rows': sum(info.get('row_count', 0) for info in backup_info.values()),
            'backup_files': []
        }
        
        # åˆ—å‡ºæ‰€æœ‰å¤‡ä»½æ–‡ä»¶
        for root, dirs, files in os.walk(self.backup_dir):
            for file in files:
                file_path = os.path.join(root, file)
                manifest['backup_files'].append({
                    'name': file,
                    'path': file_path,
                    'size': os.path.getsize(file_path)
                })
        
        # ä¿å­˜æ¸…å•
        manifest_file = os.path.join(self.backup_dir, 'backup_manifest.json')
        with open(manifest_file, 'w', encoding='utf-8') as f:
            json.dump(manifest, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“‹ å¤‡ä»½æ¸…å•å·²åˆ›å»º: {manifest_file}")
        return manifest_file
    
    async def run_backup(self):
        """è¿è¡Œå®Œæ•´å¤‡ä»½æµç¨‹"""
        logger.info("ğŸ’¾ å¼€å§‹è¿ç§»å‰æ•°æ®åº“å¤‡ä»½...")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        self.create_backup_directory()
        
        # éœ€è¦å¤‡ä»½çš„è¡¨
        tables_to_backup = [
            'shop_products', 
            'shop_categories', 
            'shop_orders', 
            'shop_order_items',
            'shop_carts',
            'shop_cart_items', 
            'shop_product_reviews',
            'shop_product_favorites'
        ]
        
        backup_info = {}
        successful_backups = 0
        
        try:
            async with self.SessionLocal() as session:
                for table_name in tables_to_backup:
                    logger.info(f"\nğŸ“¦ å¤‡ä»½è¡¨: {table_name}")
                    
                    try:
                        # å¤‡ä»½è¡¨ç»“æ„
                        column_count = await self.backup_table_structure(session, table_name)
                        
                        # å¤‡ä»½è¡¨æ•°æ®
                        row_count = await self.backup_table_data(session, table_name)
                        
                        # åˆ›å»ºè¡¨å‰¯æœ¬
                        backup_table = await self.create_table_backup_copy(table_name)
                        
                        # å¤‡ä»½ç´¢å¼•å’Œçº¦æŸ
                        metadata_count = await self.backup_indexes_and_constraints(session, table_name)
                        
                        # éªŒè¯å¤‡ä»½
                        verification_success = await self.verify_backup(session, table_name)
                        
                        backup_info[table_name] = {
                            'row_count': row_count,
                            'column_count': column_count,
                            'metadata_count': metadata_count,
                            'backup_table': backup_table,
                            'verification_success': verification_success,
                            'backup_time': datetime.now().isoformat()
                        }
                        
                        if verification_success:
                            successful_backups += 1
                            logger.info(f"âœ… {table_name} å¤‡ä»½æˆåŠŸ")
                        else:
                            logger.warning(f"âš ï¸ {table_name} å¤‡ä»½éªŒè¯å¤±è´¥")
                    
                    except Exception as e:
                        logger.error(f"âŒ å¤‡ä»½è¡¨ {table_name} å¤±è´¥: {e}")
                        backup_info[table_name] = {
                            'error': str(e),
                            'backup_time': datetime.now().isoformat()
                        }
            
            # åˆ›å»ºå¤‡ä»½æ¸…å•
            manifest_file = await self.create_backup_manifest(backup_info)
            
            end_time = datetime.now()
            duration = end_time - start_time
            
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š å¤‡ä»½ç»“æœæ±‡æ€»:")
            logger.info(f"   æ€»è¡¨æ•°: {len(tables_to_backup)}")
            logger.info(f"   æˆåŠŸå¤‡ä»½: {successful_backups}")
            logger.info(f"   å¤±è´¥å¤‡ä»½: {len(tables_to_backup) - successful_backups}")
            logger.info(f"   æ€»è¡Œæ•°: {sum(info.get('row_count', 0) for info in backup_info.values())}")
            logger.info(f"   å¤‡ä»½ç›®å½•: {self.backup_dir}")
            logger.info(f"   è€—æ—¶: {duration}")
            
            if successful_backups == len(tables_to_backup):
                logger.info("ğŸ‰ æ‰€æœ‰è¡¨å¤‡ä»½æˆåŠŸï¼å¯ä»¥å®‰å…¨æ‰§è¡Œè¿ç§»")
                return True
            else:
                logger.warning("âš ï¸ éƒ¨åˆ†è¡¨å¤‡ä»½å¤±è´¥ï¼Œå»ºè®®æ£€æŸ¥é—®é¢˜åé‡æ–°å¤‡ä»½")
                return False
        
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½è¿‡ç¨‹å¤±è´¥: {e}")
            return False
        
        finally:
            await self.engine.dispose()

async def main():
    """ä¸»å‡½æ•°"""
    print("è¿ç§»å‰æ•°æ®åº“å¤‡ä»½å·¥å…·")
    print("=" * 60)
    print("ğŸ“‹ æœ¬å·¥å…·å°†å¤‡ä»½ä»¥ä¸‹æ•°æ®ï¼š")
    print("   - æ‰€æœ‰å•†å“ç›¸å…³è¡¨çš„å®Œæ•´ç»“æ„å’Œæ•°æ®")
    print("   - æ‰€æœ‰è®¢å•ç›¸å…³è¡¨çš„å®Œæ•´ç»“æ„å’Œæ•°æ®")
    print("   - ç´¢å¼•ã€çº¦æŸç­‰å…ƒæ•°æ®ä¿¡æ¯")
    print("   - åˆ›å»ºè¡¨å‰¯æœ¬ç”¨äºå¿«é€Ÿæ¢å¤")
    print()
    
    # ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®åº“URL
    database_url = settings.DATABASE_URL
    if database_url.startswith("postgresql://"):
        database_url = database_url.replace("postgresql://", "postgresql+asyncpg://")
    
    backup_tool = PreMigrationBackup(database_url)
    success = await backup_tool.run_backup()
    
    if success:
        print("\nâœ… å¤‡ä»½å®Œæˆï¼ç°åœ¨å¯ä»¥å®‰å…¨æ‰§è¡Œè¿ç§»")
        print(f"ğŸ’¡ å¤‡ä»½ä½ç½®: {backup_tool.backup_dir}")
        print("ğŸ’¡ å¦‚éœ€æ¢å¤ï¼Œè¯·ä½¿ç”¨ rollback_spu_sku.py è„šæœ¬")
    else:
        print("\nâŒ å¤‡ä»½æœªå®Œå…¨æˆåŠŸï¼Œè¯·æ£€æŸ¥é”™è¯¯åé‡æ–°è¿è¡Œ")
        print("âš ï¸ å»ºè®®åœ¨å¤‡ä»½æˆåŠŸåå†æ‰§è¡Œè¿ç§»")

if __name__ == "__main__":
    asyncio.run(main())